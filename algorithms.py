# -*- coding: utf-8 -*-
"""genetic_algorithm

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xh6mfXEDEafVIPn9ODogpVJ847reuns2
"""

import pandas as pd
import plotly.graph_objs as go
import matplotlib.pyplot as plt
import random
import seaborn as sns
import itertools
import math
import numpy as np
from tqdm import tqdm
from collections import Counter
from sklearn.preprocessing import StandardScaler,  OneHotEncoder
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score, roc_auc_score
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier

initial_data = pd.read_excel("data.xlsx", sheet_name="option4", nrows=1701, usecols="A:Q")

# categorial_fields = ['Расчетный счет', 'Кредитная история (преобр)', 'Цель кредита (преобр)', 'Сберегательный счет', 'Стаж работы', 'Процентная ставка', 'Семейный статус', 'Поручитель/созаемщик', 'Продолж проживания в Германии', 'Имущество', 'Проживание', 'Количество кредитов ранее', 'Профессия', 'Количество поручителей/созаемщиков']

"""Кодирование номинальных признаков"""

data = pd.get_dummies(initial_data,prefix=['Кредитная история (преобр)', 'Цель кредита (преобр)', 'Семейный статус', 'Имущество'], columns = ['Кредитная история (преобр)', 'Цель кредита (преобр)', 'Семейный статус', 'Имущество'])
data

"""Стандартизация"""

scaler_std=StandardScaler()
data_after_scale=pd.DataFrame(
    scaler_std.fit_transform(data.drop(columns="Кредитоспособность")), columns=data.drop(columns="Кредитоспособность").columns)
data_after_scale['Кредитоспособность']=data['Кредитоспособность']
data_after_scale

Y = data_after_scale.iloc[:, -1]
X = data_after_scale.drop(["Кредитоспособность"], axis=1)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.30, random_state = 40, stratify=Y)
print(y_train.value_counts())
print(y_test.value_counts())

# Логистическая регрессия
log = LogisticRegression() 
log.fit(x_train, y_train)

print("Train Set Accuracy:",str(accuracy_score(y_train, log.predict(x_train))*100)) 
print("Test Set Accuracy:",str(accuracy_score(y_test, log.predict(x_test))*100)) 

print("Precision", precision_score(y_test, log.predict(x_test)))
print("Recall", recall_score(y_test, log.predict(x_test)))
print("ROC_AUC", roc_auc_score(y_test, log.predict_proba(x_test)[:,1]))

# SVM - классификатор
svc = SVC(kernel="poly", probability=True)
svc.fit(x_train,y_train)

print("Train set Accuracy:",str(accuracy_score(y_train,svc.predict(x_train))*100))
print("Test Set Accuracy:",str(accuracy_score(y_test,svc.predict(x_test))*100))
print("Precision", precision_score(y_test, svc.predict(x_test)))
print("Recall", recall_score(y_test, svc.predict(x_test)))
print("ROC_AUC", roc_auc_score(y_test, svc.predict_proba(x_test)[:,1]))

# Дерево решений
tree = DecisionTreeClassifier()
tree.fit(x_train, y_train)

print("Train test Accuracy:",str(accuracy_score(y_train, tree.predict(x_train))*100))
print("Test Set Accuracy:",str(accuracy_score(y_test, tree.predict(x_test))*100))
print("Precision", precision_score(y_test, tree.predict(x_test)))
print("Recall", recall_score(y_test, tree.predict(x_test)))
print("ROC_AUC", roc_auc_score(y_test, tree.predict_proba(x_test)[:,1]))

# Наивный Байесовский метод
gb = GaussianNB()
gb.fit(x_train, y_train)

print("Train test Accuracy:",str(accuracy_score(y_train, gb.predict(x_train))*100))
print("Test Set Accuracy:",str(accuracy_score(y_test, gb.predict(x_test))*100))
print("Precision", precision_score(y_test, gb.predict(x_test)))
print("Recall", recall_score(y_test, gb.predict(x_test)))
print("ROC_AUC", roc_auc_score(y_test, gb.predict_proba(x_test)[:,1]))

"""#Отбор признаков

## Генетический алгоритм
"""

#Турнир
def tournament(tournament_participants, expected_winners_number):
    winners = []
    while len(winners) < expected_winners_number:
        index_list = list(range(0, len(tournament_participants)))
        player1_index, player2_index = random.sample(index_list, 2)
        player1, player2 = tournament_participants[player1_index], tournament_participants[player2_index]
        if player1.fitness > player2.fitness:
            winners.append(player1)
            tournament_participants.pop(player1_index)
        else:
            winners.append(player2)
            tournament_participants.pop(player2_index)
    return winners

#Вспомогательные методы
def genetic_columns_to_names(solution):
  columns = []
  for index, value in enumerate(solution):
    if value != 0:
      columns.append(data_after_scale.columns[index])
  return columns

#Генетический алгоритм

# Класс хромосомы
class Chromosome:
    MUTATION_PROB = 0
    CHROMOSOME_LENGTH = 0

    def __init__(self, solution=None):
        if solution is not None:
            self.solution = solution[:]
        else:
            self.solution = self.generate_chromosome()
        self.fitness = self.get_chromosome_fitness()

    def generate_chromosome(self):
        chromosome = list(np.random.randint(2, size=self.CHROMOSOME_LENGTH))
        return chromosome

    def get_chromosome_fitness(self):
      table=data_after_scale.copy()
      columns_to_remove = table.columns[[i for i,x in enumerate(self.solution) if x==0]]
      table.drop(columns_to_remove, axis=1, inplace=True)

      Y = table['Кредитоспособность']
      X = table.drop('Кредитоспособность', axis = 1)

      x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.30, random_state = 40, stratify=Y)

      # svc = SVC(kernel="poly")
      # svc.fit(x_train, y_train)

      # tree = DecisionTreeClassifier(random_state=112)
      # tree.fit(x_train, y_train)

      # gb = GaussianNB()
      # gb.fit(x_train, y_train)

      log = LogisticRegression() 
      log.fit(x_train, y_train)

      # return accuracy_score(y_test, svc.predict(x_test))
      return accuracy_score(y_test, log.predict(x_test))
      # return accuracy_score(y_test, gb.predict(x_test))
      # return accuracy_score(y_test, tree.predict(x_test))
        

    def mutate(self):
        if random.random() < self.MUTATION_PROB:
            index_list = list(range(0, len(self.solution)))
            first_gene_index, second_gene_index = random.sample(index_list, 2)
            self.solution[first_gene_index], self.solution[second_gene_index] = \
                self.solution[second_gene_index], self.solution[first_gene_index]
            self.fitness = self.get_chromosome_fitness()

    def __repr__(self):
        return f"\nFitness - {self.fitness}. Columns - {genetic_columns_to_names(self.solution)}"

#Класс популяции
class Population:
    CROSSOVER_PROB = 0
    POPULATION_SIZE = 0
    NUMBER_OF_CHILDREN = 0
    ELITE_PERCENTAGE = 0
    INTERMEDIATE_POPULATION = 0

    def __init__(self, population=None):
        if population is not None:
            self.population = population
        else:
            self.population = self.generate_initial_population()
        self.fitness = self.get_population_fitness()

    def generate_initial_population(self):
        population = [Chromosome() for _ in range(0, self.POPULATION_SIZE)]
        return population

    def get_population_fitness(self):
        overall_fitness = 0
        for chromosome in self.population:
            overall_fitness += chromosome.fitness / len(self.population)
        return overall_fitness

    def choose_parents(self):
        parents = random.sample(self.population, 2)
        parent1, parent2 = parents[0], parents[1]
        return parent1, parent2

    def crossover(self, parent1, parent2):
        if random.random() > self.CROSSOVER_PROB:
            return Chromosome(parent1), Chromosome(parent2)
        index_list = list(range(0, len(parent1)))
        start_index, stop_index = sorted(random.sample(index_list, 2))
        chunk1, chunk2 = parent1[start_index: stop_index + 1], parent2[start_index: stop_index + 1]
        offspring1 = parent1[:start_index] + chunk2 + parent1[stop_index+1:]
        offspring2 = parent2[:start_index] + chunk1 + parent2[stop_index+1:]
        return Chromosome(offspring1), Chromosome(offspring2)

    def generate_children(self):
        children = []
        while len(self.population + children) < self.INTERMEDIATE_POPULATION:
            parent1, parent2 = self.choose_parents()
            offspring1, offspring2 = self.crossover(parent1.solution, parent2.solution)
            offspring1.mutate()
            offspring2.mutate()
            children.extend([offspring1, offspring2])
        self.population.extend(children)
        # because we always add two children, but INTERMEDIATE_POPULATION could be odd
        if len(self.population) > self.INTERMEDIATE_POPULATION:
            self.population.pop()

    def sort_by_fitness(self):
        return sorted(self.population, key=lambda chromosome: chromosome.fitness, reverse=True)

    def survivors_selection(self):
        sorted_population = self.sort_by_fitness()
        elite_chromosomes_number = math.ceil(self.ELITE_PERCENTAGE * len(self.population))
        new_population, tournament_participants = \
            sorted_population[0: elite_chromosomes_number], sorted_population[elite_chromosomes_number:]
        new_population.extend(tournament(tournament_participants,
                                         self.POPULATION_SIZE - len(new_population)))
        return new_population

    def __repr__(self):
        return f"Fitness - {self.fitness}. Individuals - {self.population}"

#Инициализация классов
SETTINGS = {"POPULATION_SIZE": 100,
            "NUMBER_OF_CHILDREN": 50,
            "CROSSOVER_PROB": 0.9,
            "MUTATION_PROB": 0.05,
            "ELITE_PERCENTAGE": 0.05,
            "MAX_POPULATIONS": 200,
            "THRESHOLD_EPS": 0.000001}
Chromosome.CHROMOSOME_LENGTH = data.shape[1] - 1
Chromosome.MUTATION_PROB = SETTINGS["MUTATION_PROB"]
Population.POPULATION_SIZE = SETTINGS["POPULATION_SIZE"]
Population.ELITE_PERCENTAGE = SETTINGS["ELITE_PERCENTAGE"]
Population.CROSSOVER_PROB = SETTINGS["CROSSOVER_PROB"]
Population.NUMBER_OF_CHILDREN = SETTINGS["NUMBER_OF_CHILDREN"]
Population.INTERMEDIATE_POPULATION = SETTINGS["NUMBER_OF_CHILDREN"] + SETTINGS["POPULATION_SIZE"]

#Основной алгоритм
fitness_for_graph = []
def run_genetic_algorithm(max_populations, threshold_eps):
    counter = 0
    eps = math.inf
    population = Population()
    fitness_for_graph.append(population.fitness)
    while counter < max_populations and threshold_eps < eps:
        prev_population_fitness = population.get_population_fitness()
        population.generate_children()
        new_population = population.survivors_selection()
        population = Population(new_population)
        fitness_for_graph.append(population.fitness)
        eps = abs(population.get_population_fitness() - prev_population_fitness)
        counter += 1
    return population.sort_by_fitness()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# solutions = run_genetic_algorithm(SETTINGS["MAX_POPULATIONS"], SETTINGS["THRESHOLD_EPS"])
# solutions[0]

fig=go.Figure()
fig.add_scatter(x=[i for i in range(1,len(fitness_for_graph)+1)],y=list(fitness_for_graph))

"""##ADD-DEL"""

def numbers_of_columns_to_names(solution):
  columns = []
  for val in solution:
      columns.append(data_after_scale.columns[val])
  return columns

def score_for_shorten_table(columns_indexes):
    drop_columns = list(set(X.columns) - set(numbers_of_columns_to_names(columns_indexes)))
    x_train_shorten =x_train.drop(columns=drop_columns)
    x_test_shorten =x_test.drop(columns=drop_columns)

    # log = LogisticRegression(solver='lbfgs', max_iter=400) 
    # log.fit(x_train_shorten, y_train)
    log = LogisticRegression(random_state=112) 
    log.fit(x_train_shorten, y_train)
    # tree = DecisionTreeClassifier(random_state=112)
    # tree.fit(x_train_shorten, y_train)
    
    return accuracy_score(y_test, log.predict(x_test_shorten))
    # return roc_auc_score(y_test,log.predict_proba(x_test_shorten)[:,1])

def find_feature_to_add(available_features_indexes):
    best_quality = 0
    best_feature_index = None 
    for i in available_features_indexes:
        s = list(set(range(X.shape[1])) - set(available_features_indexes)).copy()
        s.append(i)
        curr_quality=score_for_shorten_table(s)
        if curr_quality>best_quality:
            best_quality = curr_quality
            best_feature_index=i
    return best_feature_index, best_quality

def find_feature_to_delete(features_ind_avail_for_delete):
  best_quality = 0
  best_feature_index = None 
  for i in features_ind_avail_for_delete:
      s = features_ind_avail_for_delete.copy()
      s.remove(i)
      current_quality=score_for_shorten_table(s)
      if current_quality>best_quality:
          best_quality = current_quality
          best_feature_index=i
  return best_feature_index, best_quality

global_quality=0 
stop_criteria=3
best_features=[]
all_features=set(range(X.shape[1]))
graph_values=[]
inner_features=[]

outer_iter = 0
while outer_iter < stop_criteria:
  inner_quality = 0
  best_inner_features = []
  add_available_features = list(all_features - set(inner_features))
  inner_iter = 0
  # add
  while len(inner_features)!=X.shape[1] and inner_iter < stop_criteria:
    add_feature, current_quality = find_feature_to_add(add_available_features)
    print("Add_feature", add_feature)
    inner_features.append(add_feature)
    add_available_features.remove(add_feature)
    graph_values.append([current_quality, "ADD"])
    if current_quality > inner_quality:
      best_inner_features = inner_features[:]
      inner_quality = current_quality
      inner_iter = 0
    else:
      inner_iter += 1
  
  inner_iter = 0
  # del
  while len(inner_features) != 0 and inner_iter < stop_criteria:
    delete_feature, current_quality = find_feature_to_delete(inner_features)
    print("Delete_feature", delete_feature)
    inner_features.remove(delete_feature)
    graph_values.append([current_quality, "DEL"])
    if current_quality > inner_quality:
      best_inner_features = inner_features[:]
      inner_quality = current_quality
      inner_iter = 0
    else:
      inner_iter += 1

  if inner_quality > global_quality:
      global_quality = inner_quality
      best_features = best_inner_features
      outer_iter = 0
  else:
      outer_iter += 1

print("Global quality", global_quality)
print(numbers_of_columns_to_names(best_features))

fig=go.Figure()
fig.add_scatter(y=list(map(lambda val: val[0], graph_values)))

"""##DFS

Сортировка столбцов по уменьшению значимости (на основании корреляции)
"""

ranged_columns = []
for column in data_after_scale.columns[:-1]:
  corr = data_after_scale[column].corr(data_after_scale['Кредитоспособность'])
  ranged_columns.append([column, abs(corr)])
ranged_columns.sort(key=lambda val: val[1], reverse=True)
ranged_columns_names = list(map(lambda val: val[0], ranged_columns))
ranged_columns_names.append("Кредитоспособность")
data_for_DFS = data_after_scale.copy()
data_for_DFS = data_for_DFS.reindex(columns=ranged_columns_names)
data_for_DFS

"""Вспомогательные функции"""

Y = data_for_DFS["Кредитоспособность"]
X = data_for_DFS.drop(["Кредитоспособность"], axis=1)
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.30, random_state = 40, stratify=Y)

def numbers_of_columns_to_names(solution) -> list:
  columns = []
  for val in solution:
      columns.append(data_after_scale.columns[val])
  return columns

def score_for_shorten_table(columns_indexes):
    drop_columns = list(set(X.columns) - set(numbers_of_columns_to_names(columns_indexes)))
    x_train_shorten =x_train.drop(columns=drop_columns)
    x_test_shorten =x_test.drop(columns=drop_columns)

    log = LogisticRegression() 
    log.fit(x_train_shorten, y_train)
    # tree = DecisionTreeClassifier() 
    # tree.fit(x_train_shorten, y_train)
    
    # return roc_auc_score(y_test,log.predict_proba(x_test_shorten)[:,1])
    return accuracy_score(y_test, log.predict(x_test_shorten))
    # return accuracy_score(y_test, tree.predict(x_test_shorten))

"""Процедура наращивания"""

NUMBER_OF_FEATURES = 30
BEST_QUALITY = { "level": 0, "quality": 0, "features": [] }
LEVEL_DIF = 1
CORRECTION_COEF = 1
def addFeatures(list_of_features, level):
  current_quality = score_for_shorten_table(list_of_features)
  if (level == NUMBER_OF_FEATURES + 1):
    return
  if ((level >= BEST_QUALITY["level"] + LEVEL_DIF) and (BEST_QUALITY["quality"]*CORRECTION_COEF > current_quality)):
    return
  # print("Current_quality", current_quality)
  # print("Features", list_of_features)
  if (current_quality > BEST_QUALITY["quality"]*CORRECTION_COEF):
    BEST_QUALITY["level"] = level
    BEST_QUALITY["quality"] = current_quality
    BEST_QUALITY["features"] = list_of_features
  available_features = list(filter(lambda val: val > max(list_of_features), list(range(0,NUMBER_OF_FEATURES))))
  for val in available_features:
    new_list = list_of_features[:]
    new_list.append(val)
    addFeatures(new_list, level + 1)

for i in tqdm(list(range(0, NUMBER_OF_FEATURES))):
  addFeatures([i], 1)
BEST_QUALITY

BEST_QUALITY

print(numbers_of_columns_to_names(BEST_QUALITY['features']))

"""## Веса для признаков (RandomForest)"""

features = data_after_scale.drop(["Кредитоспособность"], axis=1)
target = data_after_scale["Кредитоспособность"]

model = RandomForestClassifier()
model.fit(features, target)
importances = model.feature_importances_
forest_importances = pd.Series(importances, index=features.columns).sort_values(ascending=False)
forest_importances.head(30)

fig, ax = plt.subplots(figsize=(20, 10))
forest_importances.plot.bar(ax=ax)
ax.set_title("Важность признаков")
ax.set_ylabel("Процент вклада")
fig.tight_layout()

"""## Итоговый выбор признаков"""

print("Генетический алгоритм")
print(solutions[0])

print("ADD-DEL")
print(numbers_of_columns_to_names(best_features))

print("DFS")
print(numbers_of_columns_to_names(BEST_QUALITY['features']))

print("По весам")
forest_importances.head(30)

"""#Модели

"""

compareModels = pd.DataFrame()

"""По всем признакам"""

# Логистическая регрессия
log = LogisticRegression() 
log.fit(x_train, y_train)
# confusion_matrix(y_test, log.predict(x_test))
compareModels["Логистическая (все признаки)"] = [str(accuracy_score(y_train, log.predict(x_train))*100), 
                                                 str(accuracy_score(y_test, log.predict(x_test))*100), 
                                                 precision_score(y_test, log.predict(x_test)), 
                                                 recall_score(y_test, log.predict(x_test)), 
                                                 roc_auc_score(y_test,log.predict_proba(x_test)[:,1])]
compareModels = compareModels.rename(index={0: 'Train Set Accuracy', 1: 'Test Set Accuracy', 2: 'Precision', 3: 'Recall', 4: 'Roc-auc'})
compareModels["Логистическая (все признаки)"]

# SVM - классификатор
svc = SVC(kernel="poly", probability=True)
svc.fit(x_train,y_train)
# confusion_matrix(y_test, svc.predict(x_test))
compareModels["SVM классификатор (все признаки)"] = [str(accuracy_score(y_train, svc.predict(x_train))*100), 
                                      str(accuracy_score(y_test, svc.predict(x_test))*100), 
                                      precision_score(y_test, svc.predict(x_test)), 
                                      recall_score(y_test, svc.predict(x_test)), 
                                      roc_auc_score(y_test,svc.predict_proba(x_test)[:,1])]
compareModels["SVM классификатор (все признаки)"]

# Дерево решений
tree = DecisionTreeClassifier()
tree.fit(x_train, y_train)
# confusion_matrix(y_test, tree.predict(x_test))
compareModels["Дерево решений (все признаки)"] = [str(accuracy_score(y_train, tree.predict(x_train))*100), 
                                      str(accuracy_score(y_test, tree.predict(x_test))*100), 
                                      precision_score(y_test, tree.predict(x_test)), 
                                      recall_score(y_test, tree.predict(x_test)), 
                                      roc_auc_score(y_test,tree.predict_proba(x_test)[:,1])]
compareModels["Дерево решений (все признаки)"]

# Наивный Байесовский метод
gb = GaussianNB()
gb.fit(x_train, y_train)
# confusion_matrix(y_test, gb.predict(x_test))
compareModels["Наивный байесовский (все признаки)"] = [str(accuracy_score(y_train, gb.predict(x_train))*100), 
                                      str(accuracy_score(y_test, gb.predict(x_test))*100), 
                                      precision_score(y_test, gb.predict(x_test)), 
                                      recall_score(y_test, gb.predict(x_test)), 
                                      roc_auc_score(y_test, gb.predict_proba(x_test)[:,1])]
compareModels["Наивный байесовский (все признаки)"]

"""По отобранным признакам (По генетическому | DFS)"""

# Отобранные признаки по генетическому (['Расчетный счет', 'Кредитная история (преобр)', 'Цель кредита (преобр)', 'Сберегательный счет', 'Процентная ставка', 'Продолж проживания в Германии', 'Имущество', 'Проживание'])
# Отобранные признаки по ADD-DEL (['Кредитная история (преобр)', 'Расчетный счет', 'Процентная ставка', 'Цель кредита (преобр)', 'Продолж проживания в Германии', 'Имущество', 'Возраст', 'Сберегательный счет'])
# Отобранные признаки по  DFS (['Расчетный счет', 'Кредитная история (преобр)', 'Цель кредита (преобр)', 'Сберегательный счет', 'Процентная ставка', 'Продолж проживания в Германии', 'Имущество', 'Проживание'])

"""Выбор признаков"""

#DFS drop_columns = list(set(X.columns) - set(numbers_of_columns_to_names(BEST_QUALITY['features'])))
#Ниже генетический
drop_columns = list(set(X.columns) - set(['Расчетный счет', 'Сберегательный счет', 'Процентная ставка', 'Поручитель/созаемщик', 'Продолж проживания в Германии', 'Проживание', 'Количество кредитов ранее', 'Профессия', 'Количество поручителей/созаемщиков', 'Кредитная история (преобр)_2', 'Кредитная история (преобр)_4', 'Кредитная история (преобр)_5', 'Цель кредита (преобр)_1', 'Цель кредита (преобр)_3', 'Цель кредита (преобр)_4', 'Цель кредита (преобр)_5', 'Семейный статус_1', 'Семейный статус_4', 'Имущество_2', 'Имущество_3', 'Имущество_4']))
x_train_shorten =x_train.drop(columns=drop_columns)
x_test_shorten =x_test.drop(columns=drop_columns)

# Логистическая регрессия
log = LogisticRegression() 
log.fit(x_train_shorten, y_train)
# confusion_matrix(y_test, log.predict(x_test))
compareModels["Логистическая (сокр признаки)"] = [str(accuracy_score(y_train, log.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, log.predict(x_test_shorten))*100), 
                                      precision_score(y_test, log.predict(x_test_shorten)), 
                                      recall_score(y_test, log.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, log.predict_proba(x_test_shorten)[:,1])]
compareModels["Логистическая (сокр признаки)"]

# SVM - классификатор
svc = SVC(kernel="poly", probability=True)
svc.fit(x_train_shorten,y_train)
# confusion_matrix(y_test, svc.predict(x_test_shorten))
compareModels["SVM классификатор (сокр признаки)"] = [str(accuracy_score(y_train, svc.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, svc.predict(x_test_shorten))*100), 
                                      precision_score(y_test, svc.predict(x_test_shorten)), 
                                      recall_score(y_test, svc.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, svc.predict_proba(x_test_shorten)[:,1])]
compareModels["SVM классификатор (сокр признаки)"]

# Дерево решений
tree = DecisionTreeClassifier()
tree.fit(x_train_shorten, y_train)
# confusion_matrix(y_test, tree.predict(x_test_shorten))
compareModels["Дерево решений (сокр признаки)"] = [str(accuracy_score(y_train, tree.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, tree.predict(x_test_shorten))*100), 
                                      precision_score(y_test, tree.predict(x_test_shorten)), 
                                      recall_score(y_test, tree.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, tree.predict_proba(x_test_shorten)[:,1])]
compareModels["Дерево решений (сокр признаки)"]

# Наивный Байесовский метод
gb = GaussianNB()
gb.fit(x_train_shorten, y_train)
# confusion_matrix(y_test, gb.predict(x_test_shorten))
compareModels["Наивный байесовский (сокр признаки)"] = [str(accuracy_score(y_train, gb.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, gb.predict(x_test_shorten))*100), 
                                      precision_score(y_test, gb.predict(x_test_shorten)), 
                                      recall_score(y_test, gb.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, gb.predict_proba(x_test_shorten)[:,1])]
compareModels["Наивный байесовский (сокр признаки)"]

compareModels

"""#Композиции

##Бэггинг
"""

# Бэггинг для логистической
model = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=115, random_state=12)
model.fit(x_train, y_train)
compareModels["Бэггинг (Логистическая - все пр.)"] = [str(accuracy_score(y_train, model.predict(x_train))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test))*100), 
                                      precision_score(y_test, model.predict(x_test)), 
                                      recall_score(y_test, model.predict(x_test)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test)[:,1])]
compareModels["Бэггинг (Логистическая - все пр.)"]

# Бэггинг для SVM
model = BaggingClassifier(base_estimator=SVC(kernel="poly", probability=True), n_estimators=10, random_state=12)
model.fit(x_train, y_train)
compareModels["Бэггинг (SVM - все пр.)"] = [str(accuracy_score(y_train, model.predict(x_train))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test))*100), 
                                      precision_score(y_test, model.predict(x_test)), 
                                      recall_score(y_test, model.predict(x_test)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test)[:,1])]
compareModels["Бэггинг (SVM - все пр.)"]

# Бэггинг для дерева решений
model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=60, random_state=12)
model.fit(x_train, y_train)
compareModels["Бэггинг (Дерево решений - все пр.)"] = [str(accuracy_score(y_train, model.predict(x_train))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test))*100), 
                                      precision_score(y_test, model.predict(x_test)), 
                                      recall_score(y_test, model.predict(x_test)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test)[:,1])]
compareModels["Бэггинг (Дерево решений - все пр.)"]

# Бэггинг для Наивного байесовского
model = BaggingClassifier(base_estimator=GaussianNB(), n_estimators=50, random_state=12)
model.fit(x_train, y_train)
compareModels["Бэггинг (Байесовский - все пр.)"] = [str(accuracy_score(y_train, model.predict(x_train))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test))*100), 
                                      precision_score(y_test, model.predict(x_test)), 
                                      recall_score(y_test, model.predict(x_test)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test)[:,1])]
compareModels["Бэггинг (Байесовский - все пр.)"]

"""Бэггинг на отобранных признаках"""

# Бэггинг для логистической (сокр)
model = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=40, random_state=12)
model.fit(x_train_shorten, y_train)
compareModels["Бэггинг (Логистическая - сокр пр.)"] = [str(accuracy_score(y_train, model.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test_shorten))*100), 
                                      precision_score(y_test, model.predict(x_test_shorten)), 
                                      recall_score(y_test, model.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test_shorten)[:,1])]
compareModels["Бэггинг (Логистическая - сокр пр.)"]

# Бэггинг для SVM (сокр)
model = BaggingClassifier(base_estimator=SVC(kernel="poly", probability=True), n_estimators=10, random_state=12)
model.fit(x_train_shorten, y_train)
compareModels["Бэггинг (SVM - сокр пр.)"] = [str(accuracy_score(y_train, model.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test_shorten))*100), 
                                      precision_score(y_test, model.predict(x_test_shorten)), 
                                      recall_score(y_test, model.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test_shorten)[:,1])]
compareModels["Бэггинг (SVM - сокр пр.)"]

# Бэггинг для деревьев решений (сокр)
model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=12)
model.fit(x_train_shorten, y_train)
compareModels["Бэггинг (Дерево решений - сокр пр.)"] = [str(accuracy_score(y_train, model.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test_shorten))*100), 
                                      precision_score(y_test, model.predict(x_test_shorten)), 
                                      recall_score(y_test, model.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test_shorten)[:,1])]
compareModels["Бэггинг (Дерево решений - сокр пр.)"]

# Бэггинг для наивного байесовского метода (сокр)
model = BaggingClassifier(base_estimator=GaussianNB(), n_estimators=50, random_state=12)
model.fit(x_train_shorten, y_train)
compareModels["Бэггинг (Байесовский - сокр пр.)"] = [str(accuracy_score(y_train, model.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test_shorten))*100), 
                                      precision_score(y_test, model.predict(x_test_shorten)), 
                                      recall_score(y_test, model.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test_shorten)[:,1])]
compareModels["Бэггинг (Байесовский - сокр пр.)"]

"""##Случайный лес"""

# Случайный лес (все признаки)
model = RandomForestClassifier(n_estimators=60, random_state=44)
model.fit(x_train, y_train)
compareModels["Случайный лес (все пр)"] = [str(accuracy_score(y_train, model.predict(x_train))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test))*100), 
                                      precision_score(y_test, model.predict(x_test)), 
                                      recall_score(y_test, model.predict(x_test)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test)[:,1])]
compareModels["Случайный лес (все пр)"]

# Случайный лес (сокр признаки)
model = RandomForestClassifier(n_estimators=60, random_state=44)
model.fit(x_train_shorten, y_train)
compareModels["Случайный лес (сокр пр)"] = [str(accuracy_score(y_train, model.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test_shorten))*100), 
                                      precision_score(y_test, model.predict(x_test_shorten)), 
                                      recall_score(y_test, model.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test_shorten)[:,1])]
compareModels["Случайный лес (сокр пр)"]

"""##Бустинг

### Градиентный бустинг
"""

# Градиентный бустинг (все признаки)
model = GradientBoostingClassifier(max_depth=8, n_estimators=100,
                                      random_state=12, learning_rate=0.8)
model.fit(x_train, y_train)
compareModels["Градиентный бустинг (все пр)"] = [str(accuracy_score(y_train, model.predict(x_train))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test))*100), 
                                      precision_score(y_test, model.predict(x_test)), 
                                      recall_score(y_test, model.predict(x_test)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test)[:,1])]
compareModels["Градиентный бустинг (все пр)"]

# Градиентный бустинг (сокр признаки)
model = GradientBoostingClassifier(max_depth=8, n_estimators=100,
                                      random_state=12, learning_rate=1)
model.fit(x_train_shorten, y_train)
compareModels["Градиентный бустинг (сокр пр)"] = [str(accuracy_score(y_train, model.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test_shorten))*100), 
                                      precision_score(y_test, model.predict(x_test_shorten)), 
                                      recall_score(y_test, model.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test_shorten)[:,1])]
compareModels["Градиентный бустинг (сокр пр)"]

"""### ADABoost (адаптивный бустинг)"""

# ADABoost для логистической (все признаки)
model = AdaBoostClassifier(base_estimator=LogisticRegression(), n_estimators=100, random_state=12)
model.fit(x_train, y_train)
compareModels["ADABoost (Логистическая - все пр.)"] = [str(accuracy_score(y_train, model.predict(x_train))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test))*100), 
                                      precision_score(y_test, model.predict(x_test)), 
                                      recall_score(y_test, model.predict(x_test)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test)[:,1])]
compareModels["ADABoost (Логистическая - все пр.)"]

# ADABoost для SVM
model = AdaBoostClassifier(base_estimator=SVC(kernel="poly", probability=True), n_estimators=10, random_state=12)
model.fit(x_train, y_train)
compareModels["ADABoost (SVM - все пр.)"] = [str(accuracy_score(y_train, model.predict(x_train))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test))*100), 
                                      precision_score(y_test, model.predict(x_test)), 
                                      recall_score(y_test, model.predict(x_test)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test)[:,1])]
compareModels["ADABoost (SVM - все пр.)"]

# ADABoost для дерева решений
model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=8), n_estimators=60, random_state=12)
model.fit(x_train, y_train)
compareModels["ADABoost (Дерево решений - все пр.)"] = [str(accuracy_score(y_train, model.predict(x_train))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test))*100), 
                                      precision_score(y_test, model.predict(x_test)), 
                                      recall_score(y_test, model.predict(x_test)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test)[:,1])]
compareModels["ADABoost (Дерево решений - все пр.)"]

# ADABoost для Наивного байесовского
model = AdaBoostClassifier(base_estimator=GaussianNB(), n_estimators=60, random_state=12)
model.fit(x_train, y_train)
compareModels["ADABoost (Байесовский - все пр.)"] = [str(accuracy_score(y_train, model.predict(x_train))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test))*100), 
                                      precision_score(y_test, model.predict(x_test)), 
                                      recall_score(y_test, model.predict(x_test)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test)[:,1])]
compareModels["ADABoost (Байесовский - все пр.)"]

# ADABoost для логистической (сокр)
model = AdaBoostClassifier(base_estimator=LogisticRegression(), n_estimators=100, random_state=12)
model.fit(x_train_shorten, y_train)
compareModels["ADABoost (Логистическая - сокр пр.)"] = [str(accuracy_score(y_train, model.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test_shorten))*100), 
                                      precision_score(y_test, model.predict(x_test_shorten)), 
                                      recall_score(y_test, model.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test_shorten)[:,1])]
compareModels["ADABoost (Логистическая - сокр пр.)"]

# ADABoost для SVM (сокр)
model = AdaBoostClassifier(base_estimator=SVC(kernel="poly", probability=True), n_estimators=10, random_state=12)
model.fit(x_train_shorten, y_train)
compareModels["ADABoost (SVM - сокр пр.)"] = [str(accuracy_score(y_train, model.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test_shorten))*100), 
                                      precision_score(y_test, model.predict(x_test_shorten)), 
                                      recall_score(y_test, model.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test_shorten)[:,1])]
compareModels["ADABoost (SVM - сокр пр.)"]

# ADABoost для деревьев решений (сокр)
model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=8), n_estimators=20, random_state=12)
model.fit(x_train_shorten, y_train)
compareModels["ADABoost (Дерево решений - сокр пр.)"] = [str(accuracy_score(y_train, model.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test_shorten))*100), 
                                      precision_score(y_test, model.predict(x_test_shorten)), 
                                      recall_score(y_test, model.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test_shorten)[:,1])]
compareModels["ADABoost (Дерево решений - сокр пр.)"]

# ADABoost для наивного байесовского метода (сокр)
model = AdaBoostClassifier(base_estimator=GaussianNB(), n_estimators=35, random_state=12)
model.fit(x_train_shorten, y_train)
compareModels["ADABoost (Байесовский - сокр пр.)"] = [str(accuracy_score(y_train, model.predict(x_train_shorten))*100), 
                                      str(accuracy_score(y_test, model.predict(x_test_shorten))*100), 
                                      precision_score(y_test, model.predict(x_test_shorten)), 
                                      recall_score(y_test, model.predict(x_test_shorten)), 
                                      roc_auc_score(y_test, model.predict_proba(x_test_shorten)[:,1])]
compareModels["ADABoost (Байесовский - сокр пр.)"]

compareModels.T

compareModels[['Логистическая (все признаки)', 'Логистическая (сокр признаки)', 'Бэггинг (Логистическая - все пр.)', 'Бэггинг (Логистическая - сокр пр.)', 'ADABoost (Логистическая - все пр.)', 'ADABoost (Логистическая - сокр пр.)']].T

compareModels[['SVM классификатор (все признаки)', 'SVM классификатор (сокр признаки)', 'Бэггинг (SVM - все пр.)', 'Бэггинг (SVM - сокр пр.)', 'ADABoost (SVM - все пр.)',  'ADABoost (SVM - сокр пр.)']].T

compareModels[['Дерево решений (все признаки)', 'Дерево решений (сокр признаки)', 'Бэггинг (Дерево решений - все пр.)', 'Бэггинг (Дерево решений - сокр пр.)', 'Случайный лес (все пр)', 'Случайный лес (сокр пр)',  'Градиентный бустинг (все пр)', 'Градиентный бустинг (сокр пр)', 'ADABoost (Дерево решений - все пр.)', 'ADABoost (Дерево решений - сокр пр.)']].T

compareModels[['Наивный байесовский (все признаки)', 'Наивный байесовский (сокр признаки)', 'Бэггинг (Байесовский - все пр.)', 'Бэггинг (Байесовский - сокр пр.)', 'ADABoost (Байесовский - все пр.)', 'ADABoost (Байесовский - сокр пр.)']].T

"""## Бэггинг (собственная реализация)"""

Y = data_after_scale.iloc[:, -1]
Y.loc[Y==0]=-1
X = data_after_scale.drop(["Кредитоспособность"], axis=1)
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.30, random_state = 40, stratify=Y)

#Параметры
EPS1 = 0.65
EPS2 = 0.6
COLUMNS_NUMBER = 10
LOG_MODELS_COUNT = 10
TREE_MODELS_COUNT = 10
NB_MODELS_COUNT = 10
T = LOG_MODELS_COUNT + TREE_MODELS_COUNT + NB_MODELS_COUNT

def add_model(model_type, list_of_methods):
  random_columns = random.sample(list(x_train.columns), COLUMNS_NUMBER)
  x_train_shorten = x_train[random_columns]
  x_train_inner, x_test_inner, y_train_inner, y_test_inner = train_test_split(x_train_shorten, y_train, test_size=0.3)
  method = None
  if model_type == "Tree":
    method = DecisionTreeClassifier()
    method.fit(x_train_inner, y_train_inner)
  if model_type == "Log":
    method = LogisticRegression()
    method.fit(x_train_inner, y_train_inner)
  if model_type == "NB":
    method = GaussianNB()
    method.fit(x_train_inner, y_train_inner)
  if model_type == "SVM":
    method = SVC(kernel="poly", probability=True)
    method.fit(x_train_inner, y_train_inner)

  roc_train = roc_auc_score(y_test_inner, method.predict_proba(x_test_inner)[:, 1])
  roc_test = roc_auc_score(y_train_inner, method.predict_proba(x_train_inner)[:, 1])

  if roc_train > EPS1 and roc_test > EPS2:
      list_of_methods.append([method, random_columns])

# Больше не нужно, ибо ниже взвешенное голосование

# def vote(list_of_methods):
#     models_answers = []
#     for method in list_of_methods:
#         x = x_test[method[1]]
#         models_answers.append(method[0].predict(x))

#     final_answers = []
#     number_of_methods = len(list_of_methods)
#     for i in range(y_test.shape[0]):
#         answer = 0
#         for j in range(number_of_methods):
#             answer += models_answers[j][i]

#         if answer / number_of_methods >= 0.5:
#             final_answers.append(1)
#         else:
#             final_answers.append(0)

#     return accuracy_score(y_test, final_answers)

list_of_models = []
for i in range(T):
  if i < LOG_MODELS_COUNT:
    add_model("Log", list_of_models)
  if i >= LOG_MODELS_COUNT and i < LOG_MODELS_COUNT + TREE_MODELS_COUNT:
    add_model("Tree", list_of_models)
  if i >= LOG_MODELS_COUNT + TREE_MODELS_COUNT:Ы
    add_model("NB", list_of_models)

# vote(list_of_models)

#Вспомогательные методы
get_quality = lambda x, y, b : sum([1 for i in range(len(y)) if y[i]*b.predict(x)[i]<0])/len(y)

models_with_weights = []
for item in list_of_models:
  quality = get_quality(X[item[1]], Y, item[0])
  models_with_weights.append({"model": item[0], "features": item[1], "weight": math.log((1-quality)/quality)})
df=pd.DataFrame.from_dict(models_with_weights)
df['norm weight']=[i/sum(df['weight']) for i in df['weight']]
df

# взвешенное голосование
weighted_voting=0
for index, item in enumerate(list_of_models):
  weighted_voting+=item[0].predict(X[item[1]])*df['norm weight'][index]

#процент ошибок (чтобы вывести accuracy - (1 - sum([1 for i in range(len(Y)) if Y[i]*weighted_voting[i]<0])/len(Y)))
sum([1 for i in range(len(Y)) if Y[i]*weighted_voting[i]<0])/len(Y)



"""Второй способ расчета весов"""

predicted_by_models=pd.DataFrame()
for index, item in enumerate(list_of_models):
  predicted_by_models[f'model{index}']=item[0].predict(X[item[1]])
predicted_by_models

df['weight2']=LogisticRegression().fit(predicted_by_models,Y).coef_[0]
weights=[]
for weight in df['weight2']:
  if weight < 0: 
    weights.append(0)
  else: 
    weights.append(weight)
df['norm weight2']=[i/sum(weights) for i in weights]

weighted_voting2=0
for index, item in enumerate(list_of_models):
  weighted_voting2+=item[0].predict(X[item[1]])*df['norm weight2'][index]

#процент ошибок (чтобы вывести accuracy - (1 - sum([1 for i in range(len(Y)) if Y[i]*weighted_voting[i]<0])/len(Y)))
sum([1 for i in range(len(Y)) if Y[i]*weighted_voting2[i]<0])/len(Y)

"""## Комитетный бустинг (собственная реализация)"""

Y.loc[Y==0]=-1

"""Вспомогательные функции"""

def build_logistic_regression(x, y):
  return LogisticRegression().fit(x, y)

def get_margins(x, y, trained_model):
  return y*trained_model.decision_function(x)

def get_quality(x, y, trained_model):
  number_of_errors = 0
  for margin in get_margins(x, y, trained_model):
    if margin < 0:
      number_of_errors += 1
  return number_of_errors

NUMBER_OF_ITERATIONS = 10
l0 = 100
l1 = 700
l2 = 800
dl = 20
def CommiteeBoosting(X, Y):
  all_models=[]
  initial_model = build_logistic_regression(X, Y)
  all_models.append(initial_model)

  rows_with_margins = []
  for index, row in X.iterrows():
     rows_with_margins.append({'x': row,
                               'y': Y[index],
                               'margin': get_margins([row], Y[index], initial_model)})
  sorted_rows_with_margins = sorted(rows_with_margins, key = lambda record : record['margin'])

  for t in range(1,NUMBER_OF_ITERATIONS):
    models = []
    for k in range(l1, l2, dl):
      borderX = [sorted_rows_with_margins[i]['x'] for i in range(len(sorted_rows_with_margins)) if (i >= l0 and i <= k)]
      borderY = [sorted_rows_with_margins[i]['y'] for i in range(len(sorted_rows_with_margins)) if (i >= l0 and i <= k)]
      models.append({'model': build_logistic_regression(borderX, borderY), 
                     'quality': get_quality(borderX, borderY, build_logistic_regression(borderX, borderY)) / len(borderX)})
      
    best_model = sorted(models, key = lambda model: model['quality'])[0]['model']

    sorted_rows_with_margins = sorted([{'x': sorted_rows_with_margins[i]['x'],
                                        'y': sorted_rows_with_margins[i]['y'],
                                        'margin': sorted_rows_with_margins[i]['margin'] + get_margins([x], Y[i], best_model)} for i,x in X.iterrows()],
                                      key = lambda row : row['margin'])
    
    all_models.append(best_model)
  return all_models

import warnings
warnings.filterwarnings("ignore")
all_models = CommiteeBoosting(X, Y)
values_for_graph = []
quality=1
for model in all_models:
  current_quality = get_quality(X, Y, model) / len(X)
  values_for_graph.append(current_quality)
  if current_quality < Q_best:
    Q_best = current_quality
    model_best = model

fig=go.Figure()
fig.add_scatter(y=list(values_for_graph))