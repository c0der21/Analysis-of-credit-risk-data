# -*- coding: utf-8 -*-
"""data_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZO5lZqo7_GZiH9oHtBPGROaAu9JQFVOu
"""

pip install kmodes

import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import calinski_harabasz_score
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
import numpy as np
from kmodes.kprototypes import KPrototypes
from sklearn.ensemble import IsolationForest

#change depending on excel
data = pd.read_excel("data.xlsx", sheet_name="option3", nrows=1001, usecols="A:Q")

"""# Данные"""

data

description_of_categorial_fields = {
    'Расчетный счет': "1 - нет расчетного счета; 2 - на счете нет денег; 3 - на счете до 1000 евро; 4 - больше 1000 евро",
    'Кредитная история (преобр)': "1 - задержка погашения в прошлом; 2 - другие кредиты в другом банке; 3 - кредиты не взяты / все кредиты возвращены вовремя; 4 - текущие кредиты погашаются во время; 5 - все кредиты в этом банке погашены должным образом",
    'Цель кредита (преобр)': "1 - на новую машину; 2 - на подержанную машину; 3 - на мебель; 4 - на переподготовку; 5 - на остальное",
    'Сберегательный счет': "1 - неизвестно/нет сберегательного счета; 2 - <100 евро; 3 - 100 <= … < 500 евро; 4 - 500 <= … < 1000 евро; 5 - >= 1000 евро",
    'Стаж работы': "1 - безработный; 2 - работает < 1 года ; 3 - работает > 1 года и < 4 лет; 4 - работает > 4 х лет и < 7 лет; 5 - работает > 7 лет",
    'Процентная ставка': "1 - < 2,0; 2 - 2,0 <= … < 2,5; 3 - 2,5 <= … < 3,5; 4 - >= 3,5",
    'Семейный статус': "1 - мужчина не женат; 2 - женщина замужем; 3 - мужчина женат; 4 - женщина не замужем",
    'Поручитель/созаемщик': "1 - нет; 2 - поручитель; 3 - созаемщик",
    'Продолж проживания в Германии': '1 - < 1 года, 2 - 1 <= ... < 4-x лет, 3 - 4 <= ... < 7-ми лет, 4 - >= 7-ми лет',
    'Имущество': '1 - нет имущества, 2 - автомобиль или другой транспорт, 3 - социальные сбережения/страховка, 4 - недвижимость',
    'Проживание': '1 - бесплатно проживает с кем-то, 2 - арендует жилье, 3 - собственное жилье',
    'Количество кредитов ранее': '1 - 1, 2 - 2-3, 3 - 4-5, 4 - >=6',
    'Профессия': '1 - безработный/неквалифицированный не гражданин Германии; 2 - неквалифицированный гражданин Германии; 3 - квалифицированный работник; 4 - менеджер/самостоятельный предприниматель/высококвалифицированный специалист',
    'Количество поручителей/созаемщиков': '1 - от 0 до 2; 2 - от 3 или больше'
}

"""### Рассмотрение данных

"""

data.columns

data.info()

data["Сумма кредита"].describe()

data["Возраст"].describe()

columns = data.columns
for col in data.drop(['Сумма кредита', 'Возраст'],axis=1).columns:
  print(data[col].value_counts())
  print('##################################')

"""Гистограммы для категориальных признаков"""

for col in data.drop(['Сумма кредита', 'Возраст', 'Кредитоспособность'],axis=1).columns:
  fig, ax = plt.subplots(1,1,figsize = (8,10))
  sns.countplot(x = col, hue = 'Кредитоспособность', data = data,ax=ax)
  print(description_of_categorial_fields[col])

"""Гистограммы для количественных признаков"""

fig, ax = plt.subplots(figsize = (8,5))
sns.countplot(x = pd.cut(data['Сумма кредита'],bins=5), hue = 'Кредитоспособность', data = data, ax=ax)

fig, ax = plt.subplots(figsize = (8,5))
sns.countplot(x = pd.cut(data['Возраст'],bins=[18,25,35,45,60,90]), hue = 'Кредитоспособность', data = data, ax=ax)

"""Кодировка"""

encoded_data = pd.get_dummies(data,prefix=['Кредитная история (преобр)', 'Цель кредита (преобр)', 'Семейный статус', 'Имущество'], columns = ['Кредитная история (преобр)', 'Цель кредита (преобр)', 'Семейный статус', 'Имущество'])
encoded_data

"""Стандартизация"""

scaler_std=StandardScaler()
data_after_scale=pd.DataFrame(
    scaler_std.fit_transform(encoded_data.drop(columns="Кредитоспособность")), columns=encoded_data.drop(columns="Кредитоспособность").columns)
data_after_scale['Кредитоспособность']=data['Кредитоспособность']
data_after_scale

fig, ax = plt.subplots(figsize = (30,15))
sns.heatmap(data_after_scale.corr(),annot = True,cmap = 'crest')

fig, ax = plt.subplots(figsize = (2,15))
sns.heatmap(data_after_scale.corr()[-1:].T,annot = True,cmap = 'crest')

"""К-средних"""

data_for_kmeans = data_after_scale.drop(["Кредитоспособность"], axis=1)
NUMBER_OF_CLUSTERS = 7
km = KMeans(n_clusters = NUMBER_OF_CLUSTERS, random_state = 22222)
kmeans_labels=km.fit_predict(data_for_kmeans)
Counter(kmeans_labels)

# Центры классов
# centers_of_clusters = pd.DataFrame(km.cluster_centers_, columns=data_after_scale.drop(columns='Кредитоспособность').columns)
# cluster_centers = centers_of_clusters.T
# cluster_centers

# График средних
# fig = plt.subplots(figsize = (40,8))
# for center in cluster_centers:
#   sns.lineplot(data=cluster_centers,x=cluster_centers.index,y=center)

data_with_classification=data.copy()
data_with_classification['Kmeans']=kmeans_labels
data_with_classification

"""Kmodes"""

# K-modes (K-means implementation for mixed data (categorial and numerical)) 
# https://towardsdatascience.com/clustering-algorithm-for-data-with-mixed-categorical-and-numerical-features-d4e3a48066a0
# NUMBER_OF_CLUSTERS = 7
# categorical_features_idx = []
# for col in description_of_categorial_fields.keys():
#   categorical_features_idx.append(data_for_kmeans.columns.get_loc(col))
# kproto = KPrototypes(n_clusters=NUMBER_OF_CLUSTERS , verbose=0, max_iter=20)
# kproto.fit(data_for_kmeans, categorical=categorical_features_idx)

# Cluster Centroids
# kmodes_centers_of_clusters = pd.DataFrame(kproto.cluster_centroids_, columns=data_after_scale.drop(columns='Кредитоспособность').columns)
# kmodes_cluster_centers = kmodes_centers_of_clusters.T
# kmodes_cluster_centers

# График средних
# fig = plt.subplots(figsize = (40,8))
# for center in cluster_centers:
#   sns.lineplot(data=kmodes_cluster_centers,x=cluster_centers.index,y=center)

# data_with_classification=data.copy()
# kmodes_labels = kproto.predict(data_for_kmeans, categorical=categorical_features_idx)
# data_with_classification['Kmodes']= kmodes_labels
# print(Counter(kmodes_labels))
# data_with_classification

"""Анализируем получившиеся кластеры"""

data_with_classification['Kmeans'].value_counts()
grouped = data_with_classification.groupby(['Kmeans', 'Кредитоспособность']).count()
grouped.rename(columns = {'Расчетный счет' : 'Количество'}, inplace = True)
grouped.iloc[:, [0]]

df2 = data_with_classification.groupby(['Kmeans', 'Кредитоспособность']).agg({'Кредитоспособность': 'count'})
df3 = df2.groupby(level=0).apply(lambda x:100 * x / float(x.sum()))
df3

forthCluster = data_with_classification[data_with_classification['Kmeans'] == 4]
for column in description_of_categorial_fields.keys():
  print(column)
  print("Мода", data_with_classification[column].value_counts().idxmax())
print("Средняя сумма кредита", data_with_classification["Сумма кредита"].mean())
print("Средний возраст", data_with_classification["Возраст"].mean())
print(description_of_categorial_fields)

fig = plt.subplots(1,1,figsize = (8,10))
sns.countplot(x = data_with_classification['Kmeans'], hue = 'Кредитоспособность', data = data)

"""Ящик с усами"""

boxplot = data_after_scale.boxplot(column=["Сумма кредита", "Возраст"])
boxplot
# data_after_scale.query('`Сумма кредита` < 0.8').boxplot('Сумма кредита')

"""Изолирующий лес"""

data_for_isolationForest = data_after_scale.drop(["Кредитоспособность"], axis=1)
print("Shape before", data_for_isolationForest.shape)
iso = IsolationForest()
y_outliers = iso.fit_predict(data_for_isolationForest)
data_for_isolationForest['anomaly']=y_outliers
data_for_isolationForest

for i in range(len(y_outliers)):
    if y_outliers[i] == -1:
        data_for_isolationForest.drop(i, inplace = True)
print("Shape after", data_for_isolationForest.shape)

percentOfTargetClass = 0
anomalies = list(filter(lambda x: x == -1, y_outliers))
for i in range(len(y_outliers)):
  if y_outliers[i] == -1 and data_after_scale["Кредитоспособность"][i] == 0:
    percentOfTargetClass += 1/len(anomalies)
print("Процент некредитоспособных:", percentOfTargetClass)

"""DBSCAN"""

data_for_dbscan = data_for_kmeans.copy()
for eps in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:
  for min_samp in range(5,20,2):
    db = DBSCAN(eps=0.5, min_samples=min_samp).fit(data_for_dbscan)
    labels = db.labels_
    print(Counter(labels))

"""МГК"""

data_for_PCA = data_for_kmeans.copy()
NUMBER_OF_COMPONENTS = 16
pca = PCA(n_components = NUMBER_OF_COMPONENTS)

table_train = pca.fit_transform(data_for_PCA)

explained_variance = pca.explained_variance_
explained_variance_ratio = pca.explained_variance_ratio_
print(explained_variance)
print(explained_variance_ratio)
print(explained_variance_ratio.sum())

pca_inf_table = pd.DataFrame(explained_variance, columns = {'Explained variance'}, index = [f'PC_{i}' for i in range(1,NUMBER_OF_COMPONENTS+1)])
pca_inf_table['Variance ratio']=explained_variance_ratio
ratio_total = []
for i in range(NUMBER_OF_COMPONENTS):
    ratio_total.append(sum(explained_variance_ratio[0:i+1]))
pca_inf_table['Cumulative']=ratio_total
pca_inf_table

NUMBER_OF_COMPONENTS = 7
pca = PCA(n_components = NUMBER_OF_COMPONENTS)

table_train = pca.fit_transform(data_for_PCA)

explained_variance = pca.explained_variance_
explained_variance_ratio = pca.explained_variance_ratio_
print(explained_variance)
print(explained_variance_ratio)
print(explained_variance_ratio.sum())

pca_table = pd.DataFrame(table_train, columns = [f'PC_{i}' for i in range(1,NUMBER_OF_COMPONENTS+1)])
pca_table["Кредитоспособность"] = data["Кредитоспособность"]
pca_table

# fig = plt.subplots(figsize = (12,8))
# sns.scatterplot(data=pca_table, x='PC_1', y='PC_2', hue='Кредитоспособность')

comp = pca.components_
A = pd.DataFrame((np.diag(np.sqrt(explained_variance)) @ comp),columns=pca.feature_names_in_,index=[f'PC_{i}' for i in range(1,NUMBER_OF_COMPONENTS +1)])
fig, ax = plt.subplots(figsize = (15,12))
sns.heatmap(A.T,annot = True,cmap = 'BuPu')

"""Kmeans и DBSCAN для МГК

Кmeans (уже не kmodes)
"""

NUMBER_OF_CLUSTERS = 7
km = KMeans(n_clusters = NUMBER_OF_CLUSTERS, random_state = 22222)
kmeans_labels=km.fit_predict(pca_table.iloc[:,:2])
Counter(kmeans_labels)

pca_table['Kmeans']=kmeans_labels
pca_table['Кредитоспособность']=data_after_scale['Кредитоспособность']
pca_table['Kmeans'].value_counts()
grouped = pca_table.groupby(['Kmeans', 'Кредитоспособность']).count()
grouped.rename(columns = {'PC_1' : 'Количество'}, inplace = True)
grouped.iloc[:, [0]]

df2 = pca_table.groupby(['Kmeans', 'Кредитоспособность']).agg({'Кредитоспособность': 'count'})
df3 = df2.groupby(level=0).apply(lambda x:100 * x / float(x.sum()))
df3

fig = plt.subplots(figsize = (12,8))
sns.countplot(x = 'Kmeans', hue = 'Кредитоспособность', data = pca_table)

"""DBSCAN"""

for eps in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]: #
  for min_samp in range(1,20,1):
    print(eps, min_samp)
    db = DBSCAN(eps=eps, min_samples=min_samp).fit(pca_table.iloc[:,:2])
    labels = db.labels_
    print(Counter(labels))

db = DBSCAN(eps=0.7, min_samples=11).fit(pca_table.iloc[:,:2])
pca_table["DBSCAN"] = db.labels_

fig = plt.subplots(figsize = (12,8))
sns.countplot(x = 'DBSCAN', hue = 'Кредитоспособность', data = pca_table)